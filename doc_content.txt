以下を「辻の卒業研究2 実装仕様書 v1」というイメージでまとめるね。
CLIベースのAIや別ツールにそのまま渡しても動けるレベルで、決まっている内容は全部盛りします。

辻の卒業研究2 実装仕様書 v1
0. 目的とコンセプト
0.1 研究のゴール
テーマ
オノマトペから「音をどう変えるべきか」という DSP パラメータを推論できるかを検証する。
検証したい仮説
オノマトペ（例：ガンガン、サラサラ、ズシャーーッ…）には
「どの帯域をどれくらい上げる／下げる」「どれくらいアタックを強くする」
のような 一貫した音の編集ルール が埋まっているか？
シンプルなモデル（MLP）でその対応関係を学習できるか？
目標
オノマトペ → DSP制御ベクトル（10次元）
その制御ベクトルに従って入力音声に対して信号処理を行う
モデル性能を評価して、「オノマトペから編集ルールを学習する」という枠組みの妥当性を判断する
0.2 設計方針
システムは なるべくシンプル に：
エンコーダ：オノマトペ → 数値特徴ベクトル φ(l)
モデル：MLP（多層パーセプトロン）で φ(l) → DSP パラメータベクトル
DSPエンジン：推論されたパラメータに従って、淡々と信号処理
「高機能な多段構成」ではなく、
「オノマトペ → DSPパラメータ」という 1 ステップの回帰問題 として扱う。
先行研究に従い、
オノマトペは 音素列ベース で扱う（RWCP-SSD-Onomatopoeia のルールなど）
子音・母音ヒストグラムや長さ・繰り返しなどを特徴量として設計する（Ichioka 2008, Okamoto ら）

1. システム全体構成
1.1 入出力
入力
オノマトペ（カタカナ文字列）
編集対象音声（WAV 等）
出力
DSPパラメータベクトル（10次元）
そのパラメータで処理した 編集後音声
1.2 処理フロー（オンライン動作）
カタカナオノマトペを受け取る
Julius 互換ルールで 音素列 に変換
音素列→モーラ列に変換し、
各種統計から オノマトペ特徴量ベクトル φ(l)（約38次元） を算出
φ(l) を MLPモデル に入力し、
正規化された DSP パラメータ（10次元, -1〜+1） を出力
正規化パラメータを実際の dB や倍率にマッピング
EQ / コンプレッション / トランジェント / タイムストレッチを適用
編集後音声を返す

2. データセット設計
2.1 使用データ（想定）
RWCP-SSD
105 種類の音イベント、約 9,722 音声サンプル（0.5〜2.0秒）
RWCP-SSD-Onomatopoeia
各音声に対して 15 個以上のオノマトペ（合計 155,568 個）
各オノマトペに対して
自己信頼スコア（self-reported confidence）: 1〜5
他者受容スコア（others-reported acceptance）: 1〜5
→ 自信・受容スコアが高いオノマトペのみを使うことで、
「それっぽい」ペアだけを学習に利用できる。
2.2 学習サンプル構造
1 サンプル = 1 つの (音声, オノマトペ) ペア
sound_id：音源を識別するID（ファイルパス・番号など）
onomatopoeia_katakana：オノマトペ（カタカナ）
dsp_target：教師となる DSP制御ベクトル（10次元）
acceptance_score：他者受容スコア
confidence_score：自己信頼スコア
※ dsp_target の具体的な付与方法（アノテーション or 他の処理系から導く）は今後決める事項
　→ 本仕様書では「ベクトルの形式と意味」までを確定。
2.3 オノマトペのフィルタリング方針
学習に使う候補：
confidence_score >= 4 かつ acceptance_score >= 4 などのしきい値でフィルタ
スコアの使い方：
第一版：しきい値以下を削るだけ
余裕があれば：スコアを サンプル重み にして loss に反映することも可能 

3. オノマトペ前処理 & 特徴量 φ(l) の設計
3.1 カタカナ → 音素列変換
ひらがなをカタカナに統一
全角記号（ー、ッ、ン など）を正規化
Julius の音素変換ルールを実装し、
カタカナ → 音素列（子音＋母音, 特殊記号）へ変換
子音：19種 + N（撥音）+ Q（促音）など
母音：a, i, u, e, o
長音：H（長音記号）として表現
（Julius 用の変換ルールは RWCP-SSD-Onomatopoeia でも使用されている。）
3.2 モーラ単位への分割
1モーラ =
子音＋母音 (C+V)
母音のみ (V)
N（撥音）, Q（促音）, H（長音）など
例：
「ゴロゴロ」→ [go][ro][go][ro]
「ズシャーーッ」→ [zu][sha][H][H][Q]
この モーラ列 + 音素列 をもとに、以下の特徴量を計算する。

3.3 特徴量 φ(l)（約 38 次元）の定義
グループA：全体構造・繰り返し (6次元)
M：モーラ数
C_count：子音トークン数
V_count：母音トークン数
word_repeat_count：単語レベルの繰り返し回数
「ガタンガタン」→2, 「ゴロゴロゴロ」→3, 繰り返しなし→1
mora_repeat_chunk_count：同一モーラ連続の「塊数」
mora_repeat_ratio：繰り返しモーラ数 / M
グループB：長さ・アクセント (4次元)
Q_count：促音（Q）の数
H_mora_count：長音モーラ数（H または同母音連続）
H_ratio：長音モーラ数 / M
ending_is_long：語末が長音かどうか（0/1）
※ 長音による「pyu」「piiii」の違いが、音の長さやピッチの違いを表すことが先行研究で言及されている。
グループC：母音ヒストグラム (5次元)
v_a_count
v_i_count
v_u_count
v_e_count
v_o_count
Ichioka らは、子音 19 + 母音 5 の頻度ベクトルでオノマトペの「音象徴」を扱っている。
ここでは母音を 5 次元で持ち、Sharpness / SpectralCentroid との関係分析などに使えるようにする。
グループD：子音カテゴリ・ヒストグラム (6次元)
子音を以下6カテゴリにまとめる：
無声破裂音：p, t, k, (py, ty, ky…)
有声破裂音：b, d, g
無声音摩擦音：s, sh, f, h
有声音摩擦音：z, j
鼻音：m, n, N
流音・半母音：r, w, y
対応する特徴量：
cons_voiceless_plosive_count
cons_voiced_plosive_count
cons_voiceless_fric_count
cons_voiced_fric_count
cons_nasal_count
cons_approximant_count
グループE：子音比率のサマリ (3次元)
obstruent_ratio：
(破裂音 + 摩擦音のトークン数) / C_count
voiced_cons_ratio：
(有声破裂音 + 有声音摩擦音) / C_count
nasal_ratio：
鼻音トークン数 / C_count
グループF：位置情報（語頭・語末） (12次元)
語頭・語末のモーラに含まれる子音カテゴリをワンホットで表現：
25–30. 語頭子音カテゴリ（6次元ワンホット）
31–36. 語末子音カテゴリ（6次元ワンホット）
starts_with_vowel（0/1）
ends_with_vowel（0/1）

4. DSPパラメータ設計（出力空間）
4.1 出力ベクトルの定義
モデルが出力する 10次元のDSPパラメータベクトル を以下のように定義する：
gain : 音量
compression : ダイナミクス（圧縮量）
eq_sub : 超低域 (80Hz)
eq_low : 低域 (250Hz)
eq_mid : 中域 (1kHz)
eq_high : 高域 (4kHz)
eq_presence : 超高域 (10kHz)
transient_attack : アタック
transient_sustain : サスティン
time_stretch : 長さ（タイムストレッチ倍率）
4.2 正規化スケール（学習空間）
モデルの出力空間は 各次元 -1.0〜+1.0 に正規化された値とする
0：ニュートラル（変化なし）
+1：最大限の「上げる／強くする」方向
-1：最大限の「下げる／弱くする」方向
この正規化ベクトルを DSP 側で実際の dB / 倍率にマッピングする。
4.3 DSP 実装側へのマッピング例
gain： -12dB〜+12dB
gain_dB = 12 * value
eq_*： -12dB〜+12dB
eq_band_dB = 12 * value
compression：
value=0：ほぼコンプなし
value>0：レシオ↑ or スレッショルド↓
value<0：弱め or エキスパンダ寄り など
time_stretch：
0.5〜2.0倍程度を想定：
stretch_ratio = 1.0 + 0.5 * value
（-1→0.5倍, 0→1.0倍, +1→1.5倍 など）
※ 具体的なマッピング式・上限値は DSP 実装と聴感に合わせてチューニング。

5. モデル設計（MLP）
5.1 主モデル：多層パーセプトロン（MLP）
入力次元：d_in = 38（オノマトペ特徴量 φ(l)）
出力次元：d_out = 10（DSP パラメータ）
基本構成：
隠れ層 1 層, ユニット数 32, 活性化関数 ReLU
出力層：線形（必要なら tanh を噛ませて [-1,1] に強制）
5.1.1 scikit-learn 実装例
from sklearn.neural_network import MLPRegressor

model = MLPRegressor(
    hidden_layer_sizes=(32,),  # 隠れ層1つ, 32ユニット
    activation="relu",
    solver="adam",
    max_iter=5000,
    random_state=42
)
入力 X: shape = (N_samples, 38)
出力 y: shape = (N_samples, 10)
model.fit(X_train, y_train) で学習
5.1.2 PyTorch 実装例（必要な場合）
import torch
import torch.nn as nn

class Onoma2DSPMLP(nn.Module):
    def __init__(self, d_in=38, d_out=10, hidden_dim=32):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(d_in, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, d_out),
            # 出力を [-1,1] に制限したい場合
            # nn.Tanh()
        )

    def forward(self, x):
        return self.net(x)
損失：nn.MSELoss()
最適化：torch.optim.Adam(model.parameters(), lr=1e-3)
5.2 線形回帰ベースライン（任意）
研究上は「線形モデルでどこまで行けるか」を見るとおいしいが、
本番システムとしては MLP のみでOK。
実装：Ridge 回帰など
from sklearn.linear_model import Ridge

lin_model = Ridge(alpha=1.0)
lin_model.fit(X_train, y_train)

6. データ前処理 & 分割
6.1 特徴量のスケーリング
入力 φ(l)：
各次元ごとに平均0・分散1に正規化（StandardScaler）
出力 y（DSPパラメータ）：
すでに [-1,1] に正規化された値を教師にする想定
→ そのままでも学習可能
もし生値で持つなら、同様に標準化して学習し、推論時に元スケールに戻す
6.2 train / val / test の分割
分割単位：sound_id（音源）単位
同じ音が train と test の両方に入らないようにする
比率：
train：70%
validation：15%
test：15%
流れ：
unique な sound_id リストを作る
シャッフルして 70/15/15 に分割
各 sound_id のサンプル（オノマトペ）をすべて該当セットに割り当て

7. 学習と評価
7.1 損失関数
MSE（Mean Squared Error）
1サンプルについて 10次元それぞれの二乗誤差を平均
バッチ全体の平均を最小化する
7.2 評価指標
MSE（test）
全10次元を平均した MSE
可能なら各次元ごとの MSE も併記
R²スコア（決定係数）
各DSPパラメータごとに R² を計算
1.0 に近いほど良い
0 以下なら「平均予測より悪い」
（オプション）方向の正解率
真値と予測値の符号（+/-）が一致している割合
「増やすべきときにちゃんと増やせているか？」を見る
7.3 学習ループのイメージ
train でモデル学習
各エポックで val の MSE を計測
ベストな val MSE をもつモデルを保存（early stopping してもよい）
最終的に test セットで MSE / R² をレポート
時間があれば線形回帰とも比較して精度差を確認

8. 実行時 CLI フロー例
CLIでユーザ入力：
onoma2dsp --onomatopoeia "ガンガン" --input input.wav --output output.wav
システム内部：
オノマトペ文字列 "ガンガン" を正規化（カタカナ etc.）
Julius ルールで音素列 → モーラ列
φ(l)（38次元）を計算し、学習時と同じScalerで正規化
学習済み MLP に φ(l) を入力し、10次元の正規化DSPパラメータを取得
それを物理パラメータ（dB, 倍率）に変換
input.wav に対して
EQ（sub/low/mid/high/presence）
コンプレッサ
トランジェントシェイパ
タイムストレッチ
を順に適用
output.wav に書き出す
ログには
入力オノマトペ
φ(l) の値
推論されたDSPパラメータ（正規化値と実値）
を出力するとデバッグしやすい。

9. 先行研究との関係（ざっくり整理）
Ichioka & Fukumoto (2008)
ひらがなオノマトペを音素列（19子音+5母音）に変換し、
子音・母音の頻度ベクトルを使って「音象徴に基づく類似度」を定義
→ 本仕様の 母音ヒストグラム + 子音カテゴリヒストグラム の元ネタ。
Ikawa & Kashino (2018, 2019)
オノマトペ生成モデル / オノマトペを介した音検索
音とオノマトペを同じ潜在空間にエンコードし、距離で検索
→ 「オノマトペ→潜在ベクトル」という発想の正当化。
Sundaram & Narayanan (2006, 2008)
オノマトペと意味ラベルを両方使って音クリップをクラスタリング・分類
→ オノマトペが音の知覚的性質をよく表すことの実証。
Okamoto et al. (2022) – Environmental Sound Extraction using Onomatopoeic Words
オノマトペ（音素列）を BiLSTM で埋め込みにし、
U-Net でマスク推定してターゲット音を抽出
→ 「オノマトペが duration / pitch / timbre を表現する」ことの根拠。
ONOMA-TO-WAVE (Okamoto et al., 2022)
オノマトペから環境音を seq2seq で生成する
「pyu」「piiii」のように長さ・母音パターンで音の長さ/高さを制御
→ 本仕様の 長音・繰り返し特徴 の設計理由。
RWCP-SSD-Onomatopoeia (Okamoto et al., 2020)
9,722 音声 × 155,568 オノマトペ
自己信頼・他者受容スコア付き
→ 本システムで利用するオノマトペデータの基礎。

10. 未確定事項リスト（実装時に詰めるべき点）
この仕様書は「決まった実装方針」をまとめたものだが、
意図的に まだ自由度を残している部分 がある。
dsp_target（教師ラベル）の具体的付け方
手動でオノマトペごとに 10次元ベクトルを設計するか
既存のエフェクトチェーンから逆算するか
差分（before→after）のラベルにするか など
オノマトペのフィルタしきい値
confidence, acceptance のしきい値はいくつにするか（例：4以上）
DSP の実際のマッピングレンジ
EQ ±12dB にするのか ±6dB にするのか
time_stretch のレンジ（0.5〜2.0かそれ以上か）
MLPの細かいハイパーパラメータ
隠れ層を 2層にするか（例：64→32）
学習率・バッチサイズ・エポック数
評価の合格ライン
「このR²ならオノマトペ→DSPの関係は「ある」と言える」などの基準

この仕様書をそのまま CLI ベースの AI に渡せば、
どんな特徴量を作れば良いか
どんな MLP を組めば良いか
どんな DSP ベクトルを出せば良いか
まで一通り共有できるはずです。
もし次のステップとして、
実際に Python で φ(l) を計算するコードの雛形
教師ラベル dsp_target を手作業でつけるためのテンプレート
などが必要なら、それ専用のスクリプト設計も一緒に書けます。
